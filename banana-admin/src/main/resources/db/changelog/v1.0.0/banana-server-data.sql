-- liquibase formatted sql

-- changeset whoiszxl:1
-- comment 初始化server表数据

INSERT INTO `ops_project` VALUES (1, 'aliyun秒杀项目', '模拟承载每秒1W请求的并发项目', '<h2>秒杀项目文档</h2><blockquote>根据此步骤一步一步操作，还原一个真实的秒杀业务场景</blockquote><p><br></p><h3>1. 环境准备</h3><p><br></p><h3>2. 项目上传</h3><p><br></p><h3>3. 压力测试</h3><p><br></p><h3>4. 结果总结</h3>', 2, '{\"time\": 1, \"period\": \"1\", \"zoneId\": \"random\", \"imageId\": \"centos_7_6_x64_20G_alibase_20211130.vhd\", \"password\": \"Banana1228\", \"regionId\": \"cn-shenzhen\", \"vSwitchId\": \"vsw-wz9dyftqehqk9w67ehcvm\", \"periodUnit\": \"Hourly\", \"ioOptimized\": \"optimized\", \"instanceName\": \"banana\", \"instanceType\": \"ecs.sn1.medium\", \"spotStrategy\": \"SpotWithPriceLimit\", \"uniqueSuffix\": false, \"spotPriceLimit\": 0.1, \"systemDiskSize\": \"40\", \"securityGroupId\": \"sg-wz9131jvnj7vlwa5x4dg\", \"instanceQuantity\": 1, \"instanceChargeType\": \"PostPaid\", \"internetChargeType\": \"PayByTraffic\", \"systemDiskCategory\": \"cloud_efficiency\", \"internetMaxBandwidthOut\": 50, \"securityEnhancementStrategy\": \"Active\"}', 1, 1, 0, 'banana', 'banana', '2023-03-11 01:55:45', '2023-03-12 12:41:23');
INSERT INTO `ops_project` VALUES (2, 'qcloud秒杀项目', '模拟承载每秒1W请求的并发项目', '<h2>秒杀项目文档</h2><blockquote>根据此步骤一步一步操作，还原一个真实的秒杀业务场景</blockquote><p><br></p><h3>1. 环境准备</h3><p><br></p><h3>2. 项目上传</h3><p><br></p><h3>3. 压力测试</h3><p><br></p><h3>4. 结果总结</h3>', 3, '{\"vpcId\": \"vpc-99u5p57b\", \"zone1\": \"ap-nanjing\", \"zone2\": \"ap-nanjing-1\", \"imageId\": \"img-l8og963d\", \"diskSize\": 40, \"diskType\": \"CLOUD_BSSD\", \"endpoint\": \"cvm.ap-nanjing.tencentcloudapi.com\", \"hostName\": \"banana\", \"maxPrice\": \"1000\", \"password\": \"Banana1228\", \"subnetId\": \"subnet-7f5jx24o\", \"asVpcGateway\": false, \"instanceType\": \"SA2.MEDIUM4\", \"instanceCount\": 3, \"ipv6AddressCount\": 0, \"publicIpAssigned\": true, \"securityGroupIds\": [\"sg-47kihmow\"], \"instanceChargeType\": \"SPOTPAID\", \"internetChargeType\": \"TRAFFIC_POSTPAID_BY_HOUR\", \"disableApiTermination\": false, \"internetMaxBandwidthOut\": 50, \"runMonitorServiceEnabled\": true, \"runSecurityServiceEnabled\": true, \"runAutomationServiceEnabled\": true}', 1, 1, 0, 'banana', 'banana', '2023-03-11 01:55:45', '2023-03-12 11:58:07');

INSERT INTO `ops_base_config` VALUES (1, 'aliyunAccessKeyId', 'xxxx', 1, 1, 0, '', '', '2023-03-12 12:48:16', '2023-03-12 12:48:16');
INSERT INTO `ops_base_config` VALUES (2, 'aliyunAccessSecret', 'xxxx', 1, 1, 0, '', '', '2023-03-12 12:48:32', '2023-03-12 12:48:32');

INSERT INTO `ops_deploy` VALUES (6, NULL, 'JDK部署', '将JDK部署到三台服务器', 2, '33,34,35', '', 1, 1, 2, 0, '', '', '2023-03-13 20:19:44', '2023-03-18 22:29:58');
INSERT INTO `ops_deploy` VALUES (7, NULL, 'hadoop', '将hadoop部署到三台服务器', 1, '27,28,29', '', 0, 1, 1, 1, '', '', '2023-03-14 22:10:30', '2023-03-18 22:25:51');
INSERT INTO `ops_deploy` VALUES (8, NULL, 'ZooKeeper部署', '将Zookeeper部署到三台服务器', 3, '33,34,35', '', 0, 1, 2, 0, '', '', '2023-03-17 13:18:42', '2023-03-18 22:25:55');
INSERT INTO `ops_deploy` VALUES (9, NULL, 'Kafka部署', '将Kafka部署到三台服务器', 4, '34,33,35', '', 0, 1, 2, 0, '', '', '2023-03-17 17:15:27', '2023-03-18 22:25:56');
INSERT INTO `ops_deploy` VALUES (10, NULL, 'ES部署', '将ES部署到三台服务器', 6, '33,34,35', '', 0, 1, 2, 0, '', '', '2023-03-18 16:59:33', '2023-03-18 22:25:56');
INSERT INTO `ops_deploy` VALUES (11, NULL, 'Kibana部署', '将Kibana单机部署', 7, '33', '', 0, 1, 2, 0, '', '', '2023-03-18 21:11:07', '2023-03-18 22:26:00');

INSERT INTO `ops_script` VALUES (1, 'ssh_no_pass_login.sh', 'SSH免密登录', '/opt/banana/shell', '#!/bin/bash\r\n\r\n# 读取输入的参数\r\ndata=$1\r\n\r\n#1.检测expect服务是否存在，不存在则使用yum安装expect\r\nexpectIsExists=`rpm -qa | grep expect`\r\nif [ -z $expectIsExists ]\r\nthen\r\n     yum -y install expect\r\nfi\r\n\r\n#2.密钥对不存在则创建密钥\r\n[ ! -f /root/.ssh/id_rsa.pub ] && ssh-keygen -t rsa -P \"\" -f /root/.ssh/id_rsa\r\n\r\n# 将一行数据切分为数组\r\nIFS=\"|\" read -ra dataArr <<< \"$data\"\r\n\r\n# 输出数组中的每个元素\r\nfor item in \"${dataArr[@]}\"; do\r\n  IFS=\"~\" read -r hostname password <<< \"$item\"\r\n  echo \"正在配置免密登录: hostname:$hostname  password:$password\"\r\n\r\n  # 复制公钥到其他主机并实现自动输入yes和password\r\n  expect <<EOF\r\n       spawn ssh-copy-id $hostname\r\n       expect {\r\n               \"yes/no\" { send \"yes\\n\";exp_continue } \r\n               \"password\" { send \"$password\\n\";exp_continue }\r\n               eof\r\n       }\r\nEOF\r\n\r\ndone', '输入主机名和主机密码可以实现SSH免密登录，以\"~\"符号分隔，例如：banana102~Banana1228\r\n如果需要对多台主机配置SSH免密登录，则以\"|\"符号进行分隔，例如：banana102~Banana1228|banana103~Banana1228|banana104~Banana1228', 1, 1, 0, '', '', '2023-03-14 14:09:13', '2023-03-14 15:51:37');
INSERT INTO `ops_script` VALUES (2, 'xsync.sh', '文件同步工具', '/opt/banana/shell', '#!/bin/bash\r\n#1. 判断参数个数\r\nif [ $# -lt 2 ]\r\nthen\r\n  echo Not Enough Arguement!\r\n  exit;\r\nfi\r\n#2. 遍历从参数传入的机器\r\nfor host in $1\r\ndo\r\n  #3. 遍历除第一个以外的目录参数\r\n  for file in ${@:2}\r\n  do\r\n    #4. 判断文件是否存在\r\n    if [ -e $file ]\r\n    then\r\n      #5. 获取父目录\r\n      pdir=$(cd -P $(dirname $file); pwd)\r\n      #6. 获取当前文件的名称\r\n      fname=$(basename $file)\r\n      ssh $host \"mkdir -p $pdir\"\r\n      rsync -av $pdir/$fname $host:$pdir\r\n    else\r\n      echo $file does not exists!\r\n    fi\r\n  done\r\ndone', '在一台机器上执行，同步指定的文件夹或文件到指定的机器里', 1, 1, 0, '', '', '2023-03-16 18:00:11', '2023-03-16 18:00:11');
INSERT INTO `ops_script` VALUES (3, 'install_jdk.sh', 'JDK安装脚本', '/opt/banana/shell', '#! /bin/bash\r\n\r\n# 1.获取输入参数：组件安装包名称，组件所在路径，组件的安装路径\r\nsoftwareName=$1\r\nsoftwarePath=$2\r\ninstallPath=$3\r\n\r\n# 2.查看组件是否存在，并拿到组件安装包的绝对路径\r\nisExists=`find $softwarePath -name $softwareName`\r\n\r\n# 3.判断文件是否存在\r\nif [[ ${#isExists} -ne 0 ]];then\r\n    \r\n    # 4.判断是否已经安装，如果已经安装则删除\r\n    if [ -d $installPath ];then\r\n          rm -rf $installPath\r\n    fi\r\n\r\n    # 5.创建安装路径，并且设置777权限\r\n    mkdir $installPath && chmod -R 777 $installPath\r\n\r\n    # 6.解压JDK安装包到安装路径\r\n    tar -zxvf $isExists -C $installPath >& /dev/null\r\n\r\n    # 7.获取JAVA_HOME的路径\r\n    java_home=`find $installPath -maxdepth 1 -name \"jdk*\"`\r\n    \r\n    # 8.将JAVA_HOME配置到/etc/profile\r\n    profile=/etc/profile\r\n    sed -i \"/^export JAVA_HOME/d\" $profile\r\n    echo \"export JAVA_HOME=$java_home\" >> $profile\r\n    sed -i \"/^export PATH=\\$PATH:\\$JAVA_HOME\\/bin/d\" $profile\r\n    echo \"export PATH=\\$PATH:\\$JAVA_HOME/bin\" >> $profile\r\n    sed -i \"/^export CLASSPATH=.:\\$JAVA_HOME/d\" $profile\r\n    echo \"export CLASSPATH=.:\\$JAVA_HOME/lib/dt.jar:\\$JAVA_HOME/lib/tools.jar\" >> $profile\r\n    source /etc/profile && source /etc/profile\r\n    echo \"JDK安装成功\"\r\nelse\r\n     echo \"JDK安装包不存在\"\r\nfi\r\n', '传入组件名称，组件路径，组件安装路径进行JDK的安装', 1, 1, 0, '', '', '2023-03-16 21:47:13', '2023-03-16 21:47:13');
INSERT INTO `ops_script` VALUES (4, 'start_zookeeper.sh', 'zookeeper集群启动脚本', '/opt/banana/shell', '#!/bin/bash\r\n\r\n# 集群中机器的IP地址，按顺序写入数组中\r\nips=$1\r\n\r\n# ZooKeeper配置文件所在目录\r\nzk_dir=$2\r\n\r\n# 遍历IP地址数组，启动ZooKeeper集群\r\nfor ip in ${ips[@]}; do\r\n    echo \"Starting ZooKeeper on $ip\"\r\n    ssh $ip \"source /etc/profile;zkServer.sh start\"\r\ndone\r\n\r\n# 等待ZooKeeper集群启动完成\r\nsleep 2\r\n\r\n# 遍历IP地址数组，检查ZooKeeper集群状态\r\nfor ip in ${ips[@]}; do\r\n    echo \"Checking ZooKeeper status on $ip\"\r\n    ssh $ip \"source /etc/profile;zkServer.sh status\"\r\ndone', '传入集群IP地址批量启动ZK集群', 1, 1, 0, '', '', '2023-03-17 19:22:49', '2023-03-17 19:30:36');
INSERT INTO `ops_script` VALUES (5, 'status_zookeeper.sh', 'zookeeper集群状态查看脚本', '/opt/banana/shell', '#!/bin/bash\r\n\r\n# 集群中机器的IP地址，按顺序写入数组中\r\nips=$1\r\n\r\n# 遍历IP地址数组，检查ZooKeeper集群状态\r\nfor ip in ${ips[@]}; do\r\n    echo \"Checking ZooKeeper status on $ip\"\r\n    ssh $ip \"source /etc/profile;zkServer.sh status\"\r\ndone', '传入集群IP地址批量查看ZK集群状态', 1, 1, 0, '', '', '2023-03-17 19:22:49', '2023-03-17 19:33:30');
INSERT INTO `ops_script` VALUES (6, 'status_jps.sh', 'JPS集群状态查询', '/opt/banana/shell', '#!/bin/bash\r\n\r\n# 集群中机器的IP地址，按顺序写入数组中\r\nips=$1\r\n\r\n# 遍历IP地址数组，检查ZooKeeper集群状态\r\nfor ip in ${ips[@]}; do\r\n    echo \"Checking ZooKeeper status on $ip\"\r\n    ssh $ip \"source /etc/profile;jps\"\r\ndone', '传入集群IP地址批量执行JPS查看Java相关中间件的状态', 1, 1, 0, '', '', '2023-03-17 21:57:57', '2023-03-18 14:09:38');
INSERT INTO `ops_script` VALUES (7, 'start_kafka.sh', 'kafka集群启动', '/opt/banana/shell', '#!/bin/bash\r\n\r\n# 集群中机器的IP地址，按顺序写入数组中\r\nips=$1\r\n\r\n# 遍历IP地址数组，启动Kafka集群\r\nfor ip in ${ips[@]}; do\r\n    echo \"Starting Kafka on $ip\"\r\n    ssh $ip \"source /etc/profile;kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"\r\ndone', '传入集群IP地址批量启动kafka', 1, 1, 0, '', '', '2023-03-18 13:30:23', '2023-03-18 13:30:23');
INSERT INTO `ops_script` VALUES (8, 'start_es.sh', 'es集群启动脚本', '/opt/banana/shell', '#!/bin/bash\r\n\r\n# 定义远程主机列表\r\nips=$1\r\n\r\n# 在所有远程主机上切换到ES用户并启动Elasticsearch服务\r\nfor ip in ${ips[@]}; do\r\n    echo \"Switching to ES user on $ip...\"\r\n    ssh \"$ip\" \"sudo su - es -c \'source /etc/profile;elasticsearch -d;exit;\'\"\r\ndone', '传入集群IP地址批量启动ES', 1, 1, 0, '', '', '2023-03-18 17:11:32', '2023-03-18 20:13:58');
INSERT INTO `ops_script` VALUES (9, 'start_kibana.sh', 'kibana启动脚本', '/opt/banana/shell', ' #!/bin/bash\r\n\r\nsource /etc/profile\r\n\r\nkibana >> /tmp/kibana.log 2>&1 &', '单机启动kibana', 1, 1, 0, '', '', '2023-03-18 20:44:34', '2023-03-18 21:08:29');

INSERT INTO `ops_server` VALUES (33, 1, 1, 'banana101', '47.106.180.177', '172.18.15.81', '22', 'root', 'Banana1228', 2, 1, 1, 0, '', '', '2023-03-18 12:48:13', '2023-03-18 12:48:13');
INSERT INTO `ops_server` VALUES (34, 1, 1, 'banana102', '39.108.226.135', '172.18.15.80', '22', 'root', 'Banana1228', 2, 1, 1, 0, '', '', '2023-03-18 12:48:13', '2023-03-18 12:48:13');
INSERT INTO `ops_server` VALUES (35, 1, 1, 'banana100', '47.106.65.151', '172.18.15.82', '22', 'root', 'Banana1228', 2, 1, 1, 0, '', '', '2023-03-18 12:48:13', '2023-03-18 12:48:13');

INSERT INTO `ops_software` VALUES (1, 'hadoop', 'hadoop-3.1.3.tar.gz', '/opt/banana/software/', '/opt/banana/module/', '/etc/profile.d/ops_hadoop_env.sh', '##HADOOP_HOME\r\nexport HADOOP_HOME=/opt/banana/module/hadoop-3.1.3\r\nexport PATH=$PATH:$HADOOP_HOME/bin\r\nexport PATH=$PATH:$HADOOP_HOME/sbin', '', NULL, NULL, 1, 1, 0, '', '', '2023-03-12 16:32:49', '2023-03-12 16:32:49');
INSERT INTO `ops_software` VALUES (2, 'jdk', 'jdk-8u212-linux-x64.tar.gz', '/opt/banana/software/', '/opt/banana/module/', '/etc/profile.d/ops_jdk_env.sh', '#JAVA_HOME\nexport JAVA_HOME=/opt/banana/module/jdk1.8.0_212\nexport PATH=$PATH:$JAVA_HOME/bin', '/opt/banana/shell/install_jdk.sh', NULL, NULL, 1, 1, 0, '', '', '2023-03-12 16:33:31', '2023-03-16 22:37:25');
INSERT INTO `ops_software` VALUES (3, 'zookeeper', 'apache-zookeeper-3.5.7-bin.tar.gz', '/opt/banana/software/', '/opt/banana/module/', '/etc/profile.d/ops_zookeeper_env.sh', '#ZOOKEEPER_HOME\r\nexport ZOOKEEPER_HOME=/opt/banana/module/apache-zookeeper-3.5.7-bin\r\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin', '/opt/banana/shell/install_zookeeper.sh', '/opt/banana/shell/start_zookeeper.sh', '/opt/banana/shell/status_zookeeper.sh', 1, 1, 0, '', '', '2023-03-12 16:33:31', '2023-03-17 20:46:54');
INSERT INTO `ops_software` VALUES (4, 'kafka', 'kafka_2.11-2.4.1.tgz', '/opt/banana/software/', '/opt/banana/module/', '/etc/profile.d/ops_kafka_env.sh', '#KAFKA_HOME\r\nexport KAFKA_HOME=/opt/banana/module/kafka_2.11-2.4.1\r\nexport PATH=$PATH:$KAFKA_HOME/bin', '', '/opt/banana/shell/start_kafka.sh', '/opt/banana/shell/status_jps.sh', 1, 1, 0, '', '', '2023-03-12 16:33:31', '2023-03-18 13:23:57');
INSERT INTO `ops_software` VALUES (5, 'flume', 'apache-flume-1.9.0-bin.tar.gz', '/opt/banana/software/', '/opt/banana/module/', '/etc/profile.d/ops_flume_env.sh', '#FLUME_HOME\r\nexport FLUME_HOME=/opt/banana/module/apache-flume-1.9.0-bin\r\nexport PATH=$PATH:$FLUME_HOME/bin', '', NULL, NULL, 1, 1, 0, '', '', '2023-03-12 16:33:31', '2023-03-12 16:35:30');
INSERT INTO `ops_software` VALUES (6, 'elasticsearch', 'elasticsearch-7.13.0-linux-x86_64.tar.gz', '/opt/banana/software/', '/opt/banana/module/', '/etc/profile.d/ops_es_env.sh', '#ES_HOME\r\nexport ES_HOME=/opt/banana/module/elasticsearch-7.13.0\r\nexport PATH=$PATH:$ES_HOME/bin', '', '/opt/banana/shell/start_es.sh', '/opt/banana/shell/status_jps.sh', 1, 1, 0, '', '', '2023-03-12 16:33:31', '2023-03-18 15:57:51');
INSERT INTO `ops_software` VALUES (7, 'kibana', 'kibana-7.13.0-linux-x86_64.tar.gz', '/opt/banana/software/', '/opt/banana/module/', '/etc/profile.d/ops_kibana_env.sh', ' #KIBANA_HOME\r\nexport KIBANA_HOME=/opt/banana/module/kibana-7.13.0-linux-x86_64\r\nexport PATH=$PATH:$KIBANA_HOME/bin', ' ', '/opt/banana/shell/start_kibana.sh', '/opt/banana/shell/status_jps.sh', 1, 1, 0, '', '', '2023-03-18 20:35:36', '2023-03-18 21:13:24');

INSERT INTO `ops_software_config` VALUES (1, 3, 'zookeeper', 'zoo.cfg', '/opt/banana/module/apache-zookeeper-3.5.7-bin/conf/', '# 最小时间单位，默认2000，tickTime * other\r\ntickTime=${tickTime}\r\n# 默认10，initLimit * tickTime，在这个时间内，leader会最长等待initLimit * tickTime秒follower来同步\r\ninitLimit=${initLimit}\r\n# 默认5，也就是5*tickTime，超过这个时间没有心跳就踢掉follower\r\nsyncLimit=${syncLimit}\r\n# 存放zk数据快照\r\ndataDir=${dataDir}\r\n# 存放磁盘的事务日志\r\ndataLogDir=${dataLogDir}\r\n# 客户端连接端口\r\nclientPort=${clientPort}\r\n# 客户端最多和服务端连接的请求数\r\nmaxClientCnxns=${maxClientCnxns}', '{\"dataDir\": \"/opt/banana/module/apache-zookeeper-3.5.7-bin/zkData\", \"tickTime\": \"2000\", \"initLimit\": \"10\", \"syncLimit\": \"5\", \"clientPort\": \"2181\", \"dataLogDir\": \"/opt/banana/module/apache-zookeeper-3.5.7-bin/zkDataLog\", \"maxClientCnxns\": \"60\"}', 1, 1, 0, '', '', '2023-03-17 14:02:18', '2023-03-17 14:02:18');
INSERT INTO `ops_software_config` VALUES (2, 4, 'kafka', 'server.properties', '/opt/banana/module/kafka_2.11-2.4.1/config/', '# broker的id，每台机器需要唯一。（代码中动态添加）\r\n# broker.id=\r\n# 网络接收请求处理线程数量\r\nnum.network.threads=3\r\n# 磁盘IO处理线程数量\r\nnum.io.threads=8\r\n# 日志目录\r\nlog.dirs=/opt/banana/module/kafka_2.11-2.4.1/logs\r\n# partition的数量\r\nnum.partitions=1\r\n# zk集群连接地址\r\nzookeeper.connect=banana100:2181,banana101:2181,banana102:2181/kafka\r\n# 删除topic功能是否开启\r\ndelete.topic.enable=true\r\nsocket.send.buffer.bytes=102400\r\nsocket.receive.buffer.bytes=102400\r\nsocket.request.max.bytes=104857600\r\nnum.recovery.threads.per.data.dir=1\r\noffsets.topic.replication.factor=1\r\ntransaction.state.log.replication.factor=1\r\ntransaction.state.log.min.isr=1\r\nlog.retention.hours=168\r\nlog.segment.bytes=1073741824\r\nlog.retention.check.interval.ms=300000\r\nzookeeper.connection.timeout.ms=6000\r\ngroup.initial.rebalance.delay.ms=0', '{\"logDirs\": \"/opt/banana/module/kafka_2.11-2.4.1/logs\", \"numIoThreads\": \"8\", \"numPartitions\": \"1\", \"zookeeperConnect\": \"banana100:2181,banana101:2181,banana102:2181/kafka\", \"deleteTopicEnable\": \"true\", \"numNetworkThreads\": \"3\"}', 1, 1, 0, '', '', '2023-03-17 14:03:42', '2023-03-18 13:46:03');
INSERT INTO `ops_software_config` VALUES (3, 1, 'hadoop', 'core-site.xml', '/opt/banana/module/hadoop-3.1.3/etc/hadoop/', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<configuration>\r\n    <!-- NameNode的地址 -->\r\n    <property>\r\n        <name>fs.defaultFS</name>\r\n        <value>${fsDefaultFS}</value>\r\n    </property>\r\n    <!-- hadoop数据存储目录 -->\r\n    <property>\r\n        <name>hadoop.tmp.dir</name>\r\n        <value>${hadoopTmpDir}</value>\r\n    </property>\r\n    <!-- namenode web ui登录用户 -->\r\n    <property>\r\n        <name>hadoop.http.staticuser.user</name>\r\n        <value>${hadoopHttpStaticuserUser}</value>\r\n    </property>\r\n    <!-- 配置zxl用户允许通过代理访问的主机节点 -->\r\n    <property>\r\n        <name>hadoop.proxyuser.${hadoopHttpStaticuserUser}.hosts</name>\r\n        <value>*</value>\r\n    </property>\r\n    <!-- 配置zxl用户允许通过代理用户所属组 -->\r\n    <property>\r\n        <name>hadoop.proxyuser.${hadoopHttpStaticuserUser}.groups</name>\r\n        <value>*</value>\r\n    </property>\r\n    <!-- 配置zxl用户允许通过代理的用户-->\r\n    <property>\r\n        <name>hadoop.proxyuser.${hadoopHttpStaticuserUser}.users</name>\r\n        <value>*</value>\r\n    </property>\r\n</configuration>', '{\"fsDefaultFS\": \"hdfs://banana100:8020\", \"hadoopTmpDir\": \"/opt/banana/module/hadoop-3.1.3/data\", \"hadoopHttpStaticuserUser\": \"zxl\"}', 1, 1, 0, '', '', '2023-03-17 14:05:19', '2023-03-17 14:09:32');
INSERT INTO `ops_software_config` VALUES (4, 1, 'hadoop', 'hdfs-site.xml', '/opt/banana/module/hadoop-3.1.3/etc/hadoop/', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<configuration>\r\n	<!-- namenode web ui访问的地址 -->\r\n	<property>\r\n        <name>dfs.namenode.http-address</name>\r\n        <value>${dfsNamenodeHttpAddress}</value>\r\n    </property>\r\n    \r\n	<!-- secondary namenode web ui 访问地址 -->\r\n    <property>\r\n        <name>dfs.namenode.secondary.http-address</name>\r\n        <value>${dfsNamenodeSecondaryHttpAddress}</value>\r\n    </property>\r\n    \r\n    <!-- HDFS文件系统每个副本的数量 -->\r\n    <property>\r\n        <name>dfs.replication</name>\r\n        <value>${dfsReplication}</value>\r\n    </property>\r\n</configuration>', '{\"dfsReplication\": \"1\", \"dfsNamenodeHttpAddress\": \"banana100:9870\", \"dfsNamenodeSecondaryHttpAddress\": \"banana102:9868\"}', 1, 1, 0, '', '', '2023-03-17 14:06:17', '2023-03-17 14:09:34');
INSERT INTO `ops_software_config` VALUES (5, 1, 'hadoop', 'yarn-site.xml', '/opt/banana/module/hadoop-3.1.3/etc/hadoop/', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<configuration>\r\n	<!-- MR走shuffle -->\r\n    <property>\r\n        <name>yarn.nodemanager.aux-services</name>\r\n        <value>mapreduce_shuffle</value>\r\n    </property>\r\n    \r\n    <!-- ResourceManager的地址-->\r\n    <property>\r\n        <name>yarn.resourcemanager.hostname</name>\r\n        <value>${yarnResourcemanagerHostname}</value>\r\n    </property>\r\n    \r\n    <!-- 环境变量的继承 -->\r\n    <property>\r\n        <name>yarn.nodemanager.env-whitelist</name>\r\n        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\r\n    </property>\r\n    \r\n    <!-- yarn容器最小内存 -->\r\n    <property>\r\n        <name>yarn.scheduler.minimum-allocation-mb</name>\r\n        <value>${yarnSchedulerMinimumAllocationMb}</value>\r\n    </property>\r\n    <!-- yarn容器最大内存 -->\r\n    <property>\r\n        <name>yarn.scheduler.maximum-allocation-mb</name>\r\n        <value>${yarnSchedulerMaximumAllocationMb}</value>\r\n    </property>\r\n    \r\n    <!-- yarn容器允许管理的物理内存大小 -->\r\n    <property>\r\n        <name>yarn.nodemanager.resource.memory-mb</name>\r\n        <value>${yarnNodemanagerResourceMemoryMb}</value>\r\n    </property>\r\n    \r\n    <!-- 关闭yarn对虚拟内存的限制检查 -->\r\n    <property>\r\n        <name>yarn.nodemanager.vmem-check-enabled</name>\r\n        <value>false</value>\r\n    </property>\r\n    <!-- 开启日志聚集功能 -->\r\n    <property>\r\n        <name>yarn.log-aggregation-enable</name>\r\n        <value>true</value>\r\n    </property>\r\n    <!-- 设置日志聚集服务器地址 -->\r\n    <property>  \r\n        <name>yarn.log.server.url</name>  \r\n        <value>${yarnLogServerUrl}</value>\r\n    </property>\r\n    <!-- 设置日志保留时间为7天 -->\r\n    <property>\r\n        <name>yarn.log-aggregation.retain-seconds</name>\r\n        <value>${yarnLogAggregationRetainSeconds}</value>\r\n    </property>\r\n</configuration>', '{\"yarnLogServerUrl\": \"http://banana100:19888/jobhistory/logs\", \"yarnResourcemanagerHostname\": \"banana101\", \"yarnLogAggregationRetainSeconds\": \"604800\", \"yarnNodemanagerResourceMemoryMb\": \"7000\", \"yarnSchedulerMaximumAllocationMb\": \"7000\", \"yarnSchedulerMinimumAllocationMb\": \"1024\"}', 1, 1, 0, '', '', '2023-03-17 14:07:30', '2023-03-17 14:09:34');
INSERT INTO `ops_software_config` VALUES (6, 3, 'hadoop', 'mapred-site.xml', '/opt/banana/module/hadoop-3.1.3/etc/hadoop/', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<configuration>\r\n	<!-- 指定MapReduce程序运行在什么平台上 -->\r\n    <property>\r\n        <name>mapreduce.framework.name</name>\r\n        <value>${mapreduceFrameworkName}</value>\r\n    </property>\r\n    <!-- 历史服务器地址 -->\r\n    <property>\r\n        <name>mapreduce.jobhistory.address</name>\r\n        <value>${mapreduceJobhistoryAddress}</value>\r\n    </property>\r\n    <!-- 历史服务器web地址 -->\r\n    <property>\r\n        <name>mapreduce.jobhistory.webapp.address</name>\r\n        <value>${mapreduceJobhistoryWebappAddress}</value>\r\n    </property>\r\n</configuration>', '{\"mapreduceFrameworkName\": \"yarn\", \"mapreduceJobhistoryAddress\": \"hadoop101:10020\", \"mapreduceJobhistoryWebappAddress\": \"hadoop101:19888\"}', 1, 1, 0, '', '', '2023-03-17 14:08:05', '2023-03-17 14:08:05');
INSERT INTO `ops_software_config` VALUES (7, 1, 'hadoop', 'mapred-site.xml', '/opt/banana/module/hadoop-3.1.3/etc/hadoop/', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n<configuration>\r\n	<!-- 指定MapReduce程序运行在什么平台上 -->\r\n    <property>\r\n        <name>mapreduce.framework.name</name>\r\n        <value>${mapreduceFrameworkName}</value>\r\n    </property>\r\n    <!-- 历史服务器地址 -->\r\n    <property>\r\n        <name>mapreduce.jobhistory.address</name>\r\n        <value>${mapreduceJobhistoryAddress}</value>\r\n    </property>\r\n    <!-- 历史服务器web地址 -->\r\n    <property>\r\n        <name>mapreduce.jobhistory.webapp.address</name>\r\n        <value>${mapreduceJobhistoryWebappAddress}</value>\r\n    </property>\r\n</configuration>', '{\"mapreduceFrameworkName\": \"yarn\", \"mapreduceJobhistoryAddress\": \"banana100:10020\", \"mapreduceJobhistoryWebappAddress\": \"banana100:19888\"}', 1, 1, 0, '', '', '2023-03-17 14:08:05', '2023-03-17 14:09:35');
INSERT INTO `ops_software_config` VALUES (8, 1, 'hadoop', 'workers', '/opt/banana/module/hadoop-3.1.3/etc/hadoop/', 'banana100\r\nbanana101\r\nbanana102', NULL, 1, 1, 0, '', '', '2023-03-17 14:09:08', '2023-03-17 14:09:37');
INSERT INTO `ops_software_config` VALUES (9, 5, 'flume', 'flume-env.sh', '/opt/banana/module/apache-flume-1.9.0-bin/conf/', 'export JAVA_HOME=${javaHome}', '{\"javaHome\": \"/opt/banana/module/jdk1.8.0_212\"}', 1, 1, 0, '', '', '2023-03-17 14:10:23', '2023-03-17 14:10:23');
INSERT INTO `ops_software_config` VALUES (10, 6, 'elasticsearch', 'elasticsearch.yml', '/opt/banana/module/elasticsearch-7.13.0/config/', 'cluster.name: ${clusterName}\r\n# 代码动态添加 node.name: ${nodeName}\r\npath.data: ${pathData}\r\npath.logs: ${pathLogs}\r\nhttp.port: ${httpPort}\r\ntransport.port: ${transportPort}\r\nnetwork.host: ${networkHost}\r\ndiscovery.seed_hosts: ${discoverySeedhosts}\r\n# 代码动态添加 cluster.initial_master_nodes: ${clusterInitialMasterNodes}', '{\"httpPort\": \"9200\", \"nodeName\": \"my_node_1\", \"pathData\": \"./data\", \"pathLogs\": \"./logs\", \"clusterName\": \"my_app\", \"networkHost\": \"0.0.0.0\", \"transportPort\": \"9300\", \"discoverySeedhosts\": \"[\\\"localhost:9301\\\"]\", \"clusterInitialMasterNodes\": \"[\\\"my_node_1\\\", \\\"my_node_2\\\", \\\"my_node_3\\\"]\"}', 1, 1, 0, '', '', '2023-03-18 16:22:23', '2023-03-18 17:05:44');
INSERT INTO `ops_software_config` VALUES (11, 6, 'elasticsearch', 'jvm.options', '/opt/banana/module/elasticsearch-7.13.0/config/', '-Xms${esXmsMem}m\r\n-Xmx${esXmxMem}m', '{\"esXmsMem\": \"256\", \"esXmxMem\": \"256\"}', 1, 1, 0, '', '', '2023-03-18 16:22:23', '2023-03-18 17:05:47');
INSERT INTO `ops_software_config` VALUES (12, 7, 'kibana', 'kibana.yml', ' /opt/banana/module/kibana-7.13.0-linux-x86_64/config/', 'server.host: ${serverHost}\r\nelasticsearch.hosts: ${elasticsearchHosts}', '{\"serverHost\": \"\\\"0.0.0.0\\\"\", \"elasticsearchHosts\": \"[\\\"http://localhost:9200\\\"]\"}', 1, 1, 0, '', '', '2023-03-18 20:41:29', '2023-03-18 21:14:06');
